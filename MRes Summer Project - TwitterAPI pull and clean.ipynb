{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## MRes Thesis############\n",
    "### Installing libraries\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "print(\"Success installing libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Writing functiion to access Twitter API ##################### \n",
    "# From https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "##### Passwords etc #####\n",
    "# this line sets a TOKEN variable in the environment\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAGuQogEAAAAAZb5IMMyHKqkf7R9Roxf86bw3UsQ%3DBEr5sAMWj947VD288Lbj1XyUXVRFVU1WGWV9f4JQb4OjsggxYO' # Replace <ADD_BEARER_TOKEN> with your own token, then delete this line after running it the first time\n",
    "consumer_key = \"ZiuEdUJcu8oMS5R7f7oARJvEe\" #change this to your consumer key\n",
    "consumer_secret = \"n4xV3vA9KT50w00lMFSpLBHYjqui2uuh2HlyNbXsIS6pb1pAob\" #change this to your consumer secret\n",
    "\n",
    "access_token =  \"1126141054116405249-yBmJ4NL3lUwGJCHvDz9SQcWJaardkb\" #change this to your access token\n",
    "access_token_secret = \"Cfuv3W4W4NbLbd3pBHR5Alj1q8O73EmmRWe1CLjxTyhGk\" #change this to your access token secret\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "#creating an auth() function that has the Bearer Token from Twitter\n",
    "\n",
    "#create auth functiuon to retreive token from the environment\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "# Create headers. Define a function that takes the bearer token, pass it for auth, and return hearders we will use to access the API\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "#Then create URL. Now we can access teh API, we build the request for the endpointwe are going to use and the parameters we want to pass to the API\n",
    "    #search_url is the link of the endpoint we want to access\n",
    "\n",
    "############# Buidling the query\n",
    "### Guide from https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "# Remember if you have basic or pro access then your query can only be 512 characters long for recent search endpoint. It's 1024 for premium and enterprise. Same with core and advance operators respectively.\n",
    "# You have 3 targets to choose from: Low Traffic Neighbourhoods, 15 minute cities or frequent flier tax\n",
    "LTN_search_query =\"((Low traffic neighbourhood OR Low traffic neighbourhoods OR LTN OR LTNs OR liveable neighbourhood OR liveable neighbourhoods) lang:eng -airport)\" #See https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query for how to build a query guide\n",
    "FMC_target=(\"15-minute neighbourhood\", \"15-minute neighbourhoods\", \"20 minute neighbourhood\", \"20 minute neighbourhoods\", \"15 minute city\", \"15 minute cities\", \"20 minute city\", \"20 minute cities\")\n",
    "FFT_target=(\"frequent flyer tax\", \"frequent flyer levy\", \"progressive ticket tax\", \"frequent air miles levy\", \"air miles tax\")\n",
    "\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 450):\n",
    "    #max results is set by default at 10 but I have changed it to 100, the maximum. I am not including an end date. My keywod searhc is about LTNs, see line above\n",
    "\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from. For this project it is recent search see https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent\n",
    "   \n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword, #one query for matching tweets.\n",
    "                    'start_time': start_date,  \n",
    "                    'end_time': end_date, #this is optional so I am commenting it out\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id', \n",
    "                    'tweet.fields': 'id,text,note_tweet,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {} #unique identifier to access the next page of requests \n",
    "                    }\n",
    "    return (search_url, query_params)\n",
    "    ############# connect to endpoint #############\n",
    "\n",
    "#This creates a function that puts it all toether and connects to the endpoint\n",
    "#This sends the GET requets if it si all conrrect and resutns the JSON response\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    " ##### defining my terms\n",
    "#Inputs for the request\n",
    " # find your end time code using https://www.timestamp-converter.com/\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword= \"(Low traffic neighbourhood OR Low traffic neighbourhoods OR LTN OR LTNs OR liveable neighbourhood OR liveable neighbourhoods) lang:en -airport -bank -is:retweet\"\n",
    "#keyword is your search query, see query builder\n",
    "\n",
    "#tyring to make the start and end dates work!\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Specify the desired date (21st August 2023)\n",
    "desired_start_date = datetime(2023, 8, 21)\n",
    "# Specify the desired start hour (e.g. 10 is 10:00 AM). 0 represents midnight (12:00 AM), and 23 represents 11:00 PM. \n",
    "desired_start_hour = 0\n",
    "desired_end_hour = 22\n",
    "\n",
    "# Calculate the start and end times for that date\n",
    "start_date = desired_start_date.replace(hour=desired_start_hour, minute=0, second=0)\n",
    "end_date = desired_start_date.replace(hour=desired_end_hour, minute=0, second=0)\n",
    "#end_date = desired_date + timedelta(days=1)  # End date is the next day\n",
    "\n",
    "max_results = 10\n",
    "\n",
    "print(\"finished creating an auth function new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Collect your tweets ##############\n",
    "\n",
    "#If the returned response from the below code is 200, then the request was successful.\n",
    "url = create_url(keyword, start_date_str, end_date_str, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])\n",
    "\n",
    "#print response in a readable format\n",
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# save the tweets in a file #############\n",
    "#### as a JSON file \n",
    "from tweepy import Response\n",
    "import requests\n",
    "import os\n",
    "from flask import Flask, render_template, Response\n",
    "\n",
    "with open('response.json', 'w') as f:\n",
    "    json.dump(json_response, f)\n",
    "\n",
    "# must do this separately from your function so that it doesn't interfere with looping over requests\n",
    "# Create file\n",
    "csvFile = open(\"data1.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        #source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "\n",
    "append_to_csv(json_response, \"data1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Time to loop through the requests! V3 ########\n",
    "# This didn't work because I somehow managed to take out the for loop?? and it waswn't iterating over the time frame\n",
    "# Set your desired maximum limit\n",
    "desired_max_tweets = 150  # You can adjust this number as needed. \n",
    "#Set it as desired_max_tweet= the number you want to collect - max_results because of the way the loop works\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"(Low traffic neighbourhood OR Low traffic neighbourhoods OR LTN OR LTNs OR liveable neighbourhood OR liveable neighbourhoods) lang:en -airport -bank -is:retweet\"\n",
    "max_results = 100\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate the start and end dates for a 7-day window\n",
    "#end_date = datetime.utcnow() - timedelta(seconds=12)  # Adjust end_date to be at least 10 seconds earlier\n",
    "#start_date = end_date - timedelta(days=6)  # 6 days before the end date\n",
    "\n",
    "# Convert the dates to the required format\n",
    "#end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "#start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Specify the desired date (21st August 2023)\n",
    "desired_start_date = datetime(2023, 8, 24)\n",
    "# Specify the desired start hour (e.g. 10 is 10:00 AM). 0 represents midnight (12:00 AM), and 23 represents 11:00 PM. \n",
    "desired_start_hour = 0\n",
    "desired_end_hour = 13 # it goes from most recent tweets so you have to work backwards and change this number to avoid duplicates\n",
    "\n",
    "# Calculate the start and end times for that date\n",
    "start_date = desired_start_date.replace(hour=desired_start_hour, minute=0, second=0)\n",
    "end_date = desired_start_date.replace(hour=desired_end_hour, minute=20, second=0)\n",
    "#end_date = desired_date + timedelta(days=1)  # End date is the next day\n",
    "\n",
    "# Convert the dates to the required format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "\n",
    "# Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file and write headers\n",
    "csvFile = open(\"dataFull2408AM.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id', 'lang', 'like_count', 'quote_count', 'reply_count', 'retweet_count', 'tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "while total_tweets < desired_max_tweets:\n",
    "    print(\"Loop Start\")  # Print to indicate loop iteration start\n",
    "\n",
    "    # Inputs for each loop iteration\n",
    "    count = 0  # Counting tweets per time period\n",
    "    max_count = 450  # Max tweets per time period\n",
    "    next_token = None\n",
    "\n",
    "    # Check if max_count reached\n",
    "    while count < max_count and total_tweets < desired_max_tweets:\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_date_str, end_date_str, max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for the next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "\n",
    "        if result_count is not None and result_count > 0:\n",
    "            append_to_csv(json_response, \"dataFull2408AM.csv\")\n",
    "            count += result_count\n",
    "            total_tweets += result_count\n",
    "            print(\"Total # of Tweets added: \", total_tweets)\n",
    "            print(\"count is < than max_count(450) for this timeperiod: \", count)\n",
    "            print(\"-------------------\")\n",
    "\n",
    "        time.sleep(5)\n",
    "        print(\"Loop End for this API time period\")  # Print to indicate loop iteration end\n",
    "    print(\"-------------------\")\n",
    "    # Sleep for 15 minutes before the next iteration\n",
    "    current_time = datetime.now() # Get current time\n",
    "    next_iteration_time = current_time + timedelta(minutes=15) # Calculate time after 15 minutes\n",
    "    #print(\"Current time:\", current_time)\n",
    "    #print(\"Sleeping for 15 minutes now. Next iteration will start at:\", next_iteration_time)\n",
    "\n",
    "    #time.sleep(15*60)  # Sleep for 15 minutes\n",
    "\n",
    "print(\"Total number of results: \", total_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Trying to save as much data as I can after my mistake ###\n",
    "# I saved all my random data sets (from previously testing the code) in a single workbook\n",
    "import pandas as pd\n",
    "import re\n",
    "from emoji import demojize\n",
    "# Read the Excel file with multiple sheets\n",
    "file_path = 'alldatasofarnotduplciatedpreviously.xlsx'\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a list to store cleaned data from all sheets\n",
    "cleaned_data_list = []\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Both \\r\\n and \\n can appear in your tweets. \n",
    "    # When creating a CSV from the JSON response, however, this can cause problems. \n",
    "    #Renove \\r and repalce it with a space\n",
    "    text = re.sub(r'\\r',' ', text)\n",
    "    \n",
    "    # Remove new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove usernames (starting with @)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = demojize(text)\n",
    "\n",
    "    #Remove backslashes followed by quotes\n",
    "    text = re.sub(r'\\\\\\\"', r'\"', text)\n",
    "    \n",
    "    # Remove other special characters\n",
    "    text = re.sub(r'\\\\', '', text)\n",
    "\n",
    "    # Replace HTML character entities\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    \n",
    "    #make everything lowercase\n",
    "    text= text.lower()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Loop through each sheet in the Excel file\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    # Read the sheet into a DataFrame\n",
    "    sheet_data = excel_file.parse(sheet_name)\n",
    "    \n",
    "    # Drop columns that don't have specified titles\n",
    "    required_columns = ['author id', 'created_at', 'geo', 'id', 'lang', 'like_count', 'quote_count', 'reply_count', 'retweet_count', 'tweet']\n",
    "    cleaned_sheet_data = sheet_data[required_columns]\n",
    "    \n",
    "    # Clean the \"tweet\" column and create the \"text\" column\n",
    "    #cleaned_sheet_data['clean_tweet'] = cleaned_sheet_data['tweet'].apply(clean_text)\n",
    "\n",
    "    # Drop duplicates based on the 'tweet' column \n",
    "    cleaned_sheet_data = cleaned_sheet_data.drop_duplicates(subset='tweet', keep='first')\n",
    "    \n",
    "    # Append cleaned data to the list\n",
    "    cleaned_data_list.append(cleaned_sheet_data)\n",
    "\n",
    "# Concatenate all cleaned data into a single DataFrame\n",
    "combined_cleaned_data = pd.concat(cleaned_data_list, ignore_index=True)\n",
    "\n",
    "# Save combined cleaned data to a CSV file\n",
    "output_file_path = 'finalcombineddatasetnoduplicates.csv'\n",
    "combined_cleaned_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Combined cleaned data saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Cleaning up my data ################### V2\n",
    "# Some tips have been taken from here https://towardsdatascience.com/mistakes-to-avoid-when-using-twitter-data-for-the-first-time-304c3d0ef7a6\n",
    "# Some text will be contained in images or video, which I will not be able to access\n",
    "import pandas as pd\n",
    "import re\n",
    "import demoji\n",
    "from emoji import demojize\n",
    "from nltk.stem import PorterStemmer  # Import the stemmer\n",
    "\n",
    "demoji.download_codes() # you need to download emojis\n",
    "#When I check the data in excel and R, there are still some duplciates. I'm not sure why, and I can't seem to make them disappear here.\n",
    "#So I removed them manually in Excel and checked on python.\n",
    "#The new file is called #unique_combined_cleaned_data.csv\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"finalcombineddatasetnoduplicates_duplciatesbetweendatasetsremoved.csv\"\n",
    "data = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Both \\r\\n and \\n can appear in your tweets. \n",
    "    # When creating a CSV from the JSON response, however, this can cause problems. \n",
    "    #Renove \\r and repalce it with a space\n",
    "    text = re.sub(r'\\r',' ', text)\n",
    "    \n",
    "    # Remove new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove usernames (starting with @)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    #Remove backslashes followed by quotes\n",
    "    text = re.sub(r'\\\\\\\"', r'\"', text)\n",
    "    \n",
    "    # Remove other special characters\n",
    "    text = re.sub(r'\\\\', '', text)\n",
    "\n",
    "    # Replace HTML character entities\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Clean the \"tweet\" column and create the \"text\" column\n",
    "data['text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Stemming function using Porter stemmer\n",
    "stemmer = PorterStemmer()  # Initialize the stemmer\n",
    "\n",
    "def stem_text(text):\n",
    "    text = demojize(text) #convert emojis to text\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Duplicate the 'text' column and apply stemming\n",
    "data['stemmed_text'] = data['text'].apply(stem_text)\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "\n",
    "# Save the updated data back to the CSV file\n",
    "cleaned_csv_file_path = \"cleaned_data.csv\"\n",
    "data.to_csv(cleaned_csv_file_path, index=False,  encoding='utf-8')\n",
    "\n",
    "print(\"Cleaning complete. Data saved to cleaned_data.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Trying to remove weird encodings\n",
    "# I'm getting werid encodings because UTF-8 encodes special characters using 1 to 4 bytes. These have been split up and decoded into\n",
    "# Western, before being encoded again. They're also of variable length so I need to find a way to remove them.\n",
    "# https://stackoverflow.com/questions/31671906/how-do-i-recreate-these-special-characters-when-testing-a-website\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#These I have picked up manually by looking at tweets\n",
    "text = re.sub(\"ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢\", \"'\", text)\n",
    "text = re.sub(\"RT :\", \"\", text)\n",
    "text = re.sub(\"ÃƒÂ¢Ã‹Å“Ã‚ÂºÃƒÂ¯Ã‚Â¸Ã‚Â\", \":)\", text)\n",
    "text = re.sub(\"ÃƒÂ°Ã…Â¸Ã¢â‚¬ËœÃ¢â‚¬Â°\", \"Search instead for\", text)\n",
    "text = re.sub(\"ÃƒÂ¢Ã…â€œÃ¢â‚¬Â¦\", \"tick emoji\", text)\n",
    "text = re.sub(\"Ãƒâ€šÃ‚Â\", \"\", text)\n",
    "text = re.sub(\"ÃƒÂ°Ã…Â¸Ã‹Å“Ã‚Â£\", \":(\", text)\n",
    "\n",
    "#### Making a function to remove all other weird encodings\n",
    "import unicodedata\n",
    "#def remove_weird_sequences(text):\n",
    " #   cleaned_text = re.sub(r'[\\u0080-\\uFFFF]', '', text)\n",
    "  #  cleaned_text = re.sub(r'ÃƒÂ+', '', cleaned_text)\n",
    "    #cleaned_text = re.sub(r'ƒ+', '', cleaned_text)\n",
    "   # return cleaned_text\n",
    "\n",
    "def remove_weird_sequences(text):\n",
    "    if pd.notna(text):\n",
    "        cleaned_text = re.sub(r'[\\u0080-\\uFFFF]', '', text)\n",
    "        cleaned_text = re.sub(r'ÃƒÂ+', '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'ƒ+', '', cleaned_text)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Example usage\n",
    "text1 = \"This is a text with weird sequences ÃƒÂabcÃƒÂdef ÃƒÂghi and some punctuation: ÃƒÂ; , !\"\n",
    "cleaned_text1 = remove_weird_sequences(text1)\n",
    "print(cleaned_text1)  # Output: \"This is a text with weird sequences and some punctuation:; , !\"\n",
    "\n",
    "text2 = \"The MayorÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s car!\"\n",
    "cleaned_text2 = remove_weird_sequences(text2)\n",
    "print(cleaned_text2)  # Output: \"The Mayors car!\"\n",
    "\n",
    "\n",
    "#Time to apply it \n",
    "\n",
    "csv_file_path = \"cleaned_data.csv\"\n",
    "data = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "data['text'] = data['text'].apply(remove_weird_sequences)\n",
    "data['stemmed_text'] = data['stemmed_text'].apply(remove_weird_sequences)\n",
    "\n",
    "# Save the updated data back to the CSV file\n",
    "cleaned_csv_file_path = \"cleaned_data_utf8.csv\"\n",
    "data.to_csv(cleaned_csv_file_path, index=False,  encoding='utf-8')\n",
    "\n",
    "print(\"Cleaning complete. Data saved to cleaned_data.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Checking for duplicates\n",
    "# Check for duplicate tweet content\n",
    "duplicate_content = data[data.duplicated('text', keep=False)]\n",
    "\n",
    "# Check for duplicate tweet IDs\n",
    "duplicate_ids = data[data.duplicated('author id', keep=False)]\n",
    "\n",
    "print(\"Duplicate tweets based on content:\")\n",
    "print(duplicate_content)\n",
    "\n",
    "#print(\"\\nDuplicate tweets based on tweet IDs:\")\n",
    "#print(duplicate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Detecting moral foundations in tweets ###################\n",
    "\n",
    "#### This uses the MoralStrength Lexicon\n",
    "#### Taken from https://github.com/oaraque/moral-foundations ####\n",
    "\n",
    "# WARNING: You will get warnings about inconsistent versions of scikit learn and spaCy.\n",
    "# This is often realted to the compatibiliyt of the library version you haev installed.\n",
    "# These warnging alert you that you are using models trained wi th different versions of the library which can lead to invalid results. \n",
    "# I have tried to resolve these woarning by ipgrading my library versions and updating dependencies\n",
    "# I do n't have the time to repickle my models using the latest versions\n",
    "# You can turn off these warnings using the code below\n",
    "#import warnings\n",
    "#from sklearn.exceptions import InconsistentVersionWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "#tesing out the lexicon\n",
    "import moralstrength\n",
    "from moralstrength.moralstrength import estimate_morals\n",
    "\n",
    "#### Here is example code from the github page so you can see how it works ####\n",
    "text = \"PLS help #HASHTAG's family. No one prepares for this. They are in need of any assistance you can offer\"  \n",
    "\n",
    "result= moralstrength.string_moral_value(text, moral='care')\n",
    "print(result)\n",
    "\n",
    "from moralstrength.moralstrength import estimate_morals\n",
    "\n",
    "texts = '''My dog is very loyal to me.\n",
    "My cat is not loyal, but understands my authority.\n",
    "He did not want to break the router, he was fixing it.\n",
    "It is not fair! She cheated on the exams.\n",
    "Are you pure of heart? Because I am sure not.\n",
    "Will you take care of me? I am sad.'''\n",
    "\n",
    "texts = texts.split('\\n')\n",
    "\n",
    "result = estimate_morals(texts, process=True) # set to false if text is alredy pre-processed\n",
    "print(result)\n",
    "\n",
    "import moralstrength\n",
    "\n",
    "text = \"PLS help #HASHTAG's family. No one prepares for this. They are in need of any assistance you can offer\"  \n",
    "\n",
    "moralstrength.string_moral_value(text, moral='care')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Updating lexicon to most recent and to include liberty ###\n",
    "import pandas as pd\n",
    "# Update versiom\n",
    "from moralstrength import lexicon_use\n",
    "lexicon_use.select_version(\"latest\")\n",
    "\n",
    "#Attempting to add the liberty lexicon https://github.com/oaraque/moral-foundations/blob/master/liberty/2nd_version/cs_lexicon_final.csv\n",
    "# Load the liberty lexicon CSV file of your choice\n",
    "lexicon_file_path = \"we_lexicon_final.csv\" #\"cs_lexicon_final.csv\"  # Or \"we_lexicon_final.csv\"\n",
    "lexicon_data = pd.read_csv(lexicon_file_path)\n",
    "\n",
    "# Display the first few rows of the lexicon DataFrame\n",
    "print(lexicon_data.head())\n",
    "\n",
    "#### Trying to estimate liberty with a function of our own ####\n",
    "# Load the word embedding lexicon\n",
    "lexicon_path = 'we_lexicon_final.csv'\n",
    "lexicon = pd.read_csv(lexicon_path)\n",
    "\n",
    "def estimate_liberty(texts):\n",
    "    \"\"\"\n",
    "    Estimate Liberty moral values for a list of texts using the provided word embedding lexicon.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List of texts to estimate Liberty moral values for.\n",
    "\n",
    "    Returns:\n",
    "    - A Pandas DataFrame with the estimated Liberty moral values for each input text.\n",
    "    \"\"\"\n",
    "\n",
    "    liberty_scores = []  # List to store Liberty scores\n",
    "\n",
    "    for text in texts:\n",
    "        liberty_score = 0  # Initialize the Liberty score\n",
    "        \n",
    "        for word in text.split():  # Split text into words (adjust as needed)\n",
    "            word_data = lexicon[lexicon['word'] == word]\n",
    "            if not word_data.empty:\n",
    "                liberty_score += word_data['score'].values[0]\n",
    "        \n",
    "        liberty_scores.append(liberty_score)\n",
    "\n",
    "    # Create a DataFrame with the Liberty scores\n",
    "    estimation = pd.DataFrame({'liberty': liberty_scores})\n",
    "\n",
    "    return estimation\n",
    "\n",
    "# Example usage\n",
    "texts = [\"Freedom is essential for a thriving society.\",\n",
    "         \"Government should have limited power over individuals.\",\n",
    "         \"Liberty ensures individual rights and autonomy.\"]\n",
    "liberty_estimation = estimate_liberty(texts)\n",
    "print(liberty_estimation)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "texts = [\"I love LTN\",\n",
    "         \"Low Traffic Neighbourhood 'discourse' has made it into the local Facebook group. It's full of all the usual mad nonsense you'd come to expect, depressingly\",\n",
    "         \"We don't live in a democracy. Only the illusion if one. Net zero no one asked. LTN'S no one asked ULEZ no one asked. Speed bumps no one asked. Mass immigration no one asked.\"]\n",
    "liberty_estimation = estimate_liberty(texts)\n",
    "print(liberty_estimation)\n",
    "\n",
    "#something isn't right here because the first sentence here should not be scoring.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lexicon_use import form_text_vector\n",
    "from estimators import estimate\n",
    "\n",
    "from gsitk.preprocess import pprocess_twitter, simple, Preprocessor\n",
    "\n",
    "\n",
    "#Dont have time to make this workd so I will give up on including Liberty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moralstrength import lexicon_use\n",
    "from moralstrength.estimators import estimate,models\n",
    "from moralstrength.moral_list import moral_options_lexicon, moral_options_predictions\n",
    "from moralstrength import string_moral_values\n",
    "#text='LTNs are the wrost. Anyone who likes them is my enemy'\n",
    "#text='LTNs are the best. I love clean air'\n",
    "#text='It brought our neighbourhood closer together - we all had posters in our windows saying \"Bollards to LTNs\" and we all went on marches to the town hall together.'\n",
    "text='PLS help #HASHTAGs family. No one prepares for this. They are in need of any assistance you can offer'\n",
    "string_moral_values(text, model='unigram+freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from moralstrength import lexicon_use\n",
    "from moralstrength.estimators import estimate,models\n",
    "from moralstrength.moral_list import moral_options_lexicon, moral_options_predictions\n",
    "from moralstrength import string_moral_values\n",
    "\n",
    "\n",
    "#moralstrength.string_moral_value(text, moral='care')\n",
    "# Load your cleaned CSV file into a DataFrame\n",
    "cleaned_csv_file_path = \"cleaned_data_utf8.csv\"\n",
    "data = pd.read_csv(cleaned_csv_file_path)\n",
    "\n",
    "# Define a list of moral traits you want to estimate\n",
    "moral_traits = [\"loyalty\", \"authority\", \"purity\", \"fairness\", \"care\"]\n",
    "\n",
    "# Function to estimate moral strengths for a given tweet\n",
    "def estimate_moral_strengths(tweet):\n",
    "    moral_values = string_moral_values(tweet, model=\"unigram+freq\")\n",
    "    return moral_values\n",
    "\n",
    "# Iterate through the moral traits and create new columns for moral strengths\n",
    "for trait in moral_traits:\n",
    "    data[trait + \"_strength\"] = data['tweet'].apply(lambda x: estimate_moral_strengths(x).get(trait, None))\n",
    "# Save the updated data with individualizing and moral_present values to a new CSV file\n",
    "data_with_values_csv_file_path = \"data_with_moral_values.csv\"\n",
    "data.to_csv(data_with_values_csv_file_path, index=False)\n",
    "\n",
    "print(\"Individualizing and moral_present values added to the data. Data saved to\", data_with_values_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### V2 Detecting presenece of moral values and their strenghth in tweets ##########\n",
    "from moralstrength import estimate_morals, lexicon_morals\n",
    "# Load your cleaned CSV file into a DataFrame\n",
    "cleaned_csv_file_path = \"cleaned_data_utf8.csv\"\n",
    "data = pd.read_csv(cleaned_csv_file_path)\n",
    "\n",
    "# Estimate moral values for each tweet and create new columns for each moral trait\n",
    "def estimate_moral_values(tweet):\n",
    "    moral_values = estimate_morals([tweet], process=True)\n",
    "    return moral_values.iloc[0]\n",
    "\n",
    "moral_columns = lexicon_morals()\n",
    "for column in moral_columns:\n",
    "    data[column] = data['stemmed_text'].apply(lambda x: estimate_moral_values(x)[column] if pd.notnull(x) else None)\n",
    "\n",
    "# Calculate individualizing values for each row and create a new column\n",
    "def classify_individualizing(row):\n",
    "    if (row['fairness'] > 0) or (row['care'] > 0):\n",
    "        return 1  # Fairness or care is present\n",
    "    else:\n",
    "        return 0  # Other cases\n",
    "    \n",
    "def classify_binding(row):\n",
    "    if (row['loyalty'] > 0) or (row['authority'] > 0) or (row['purity'] > 0):\n",
    "        return 1  # Loyalty, authority, or purity is present\n",
    "    else:\n",
    "        return 0  # Other cases\n",
    "\n",
    "def classify_individualizing_cont(row):\n",
    "    fairness = row['fairness'] if not pd.isna(row['fairness']) else 0\n",
    "    care = row['care'] if not pd.isna(row['care']) else 0\n",
    "    if (fairness > 0) or (care > 0):\n",
    "        return (fairness + care)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def classify_binding_cont(row):\n",
    "    loyalty = row['loyalty'] if not pd.isna(row['loyalty']) else 0\n",
    "    authority = row['authority'] if not pd.isna(row['authority']) else 0\n",
    "    purity = row['purity'] if not pd.isna(row['purity']) else 0\n",
    "    if (loyalty > 0) or (authority > 0) or (purity > 0):\n",
    "        return (loyalty + authority + purity)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def classify_bin_individualizing(row):\n",
    "    if (row['fairness'] > 0) or (row['care'] > 0):\n",
    "        return 2  # Fairness or care is present\n",
    "    elif (row['loyalty'] > 0) or (row['authority'] > 0) or (row['purity'] > 0):\n",
    "        return 1 # Binding is present\n",
    "    else:\n",
    "        return 0  # no morals are pesent\n",
    "\n",
    "#apply these functions to the data\n",
    "data['individualising_cat'] = data.apply(classify_individualizing, axis=1)\n",
    "data['binding_cat'] = data.apply(classify_binding, axis=1)\n",
    "data['individualising_cont'] = data.apply(classify_individualizing_cont, axis=1)\n",
    "data['binding_cont'] = data.apply(classify_binding_cont, axis=1)\n",
    "data['binary_indiv'] = data.apply(classify_bin_individualizing, axis=1)\n",
    "\n",
    "# Calculate moral_present values for each row and create a new column\n",
    "def classify_moral_present(row):\n",
    "    for column in moral_columns:\n",
    "        if column != 'liberty' and row[column] > 0:\n",
    "            return 1  # At least one moral (excluding liberty) is present\n",
    "    return 0  # No moral (excluding liberty) is present\n",
    "\n",
    "data['moral_present'] = data.apply(classify_moral_present, axis=1)\n",
    "\n",
    "# Save the updated data with individualizing and moral_present values to a new CSV file\n",
    "data_with_values_csv_file_path = \"data_with_moral_values.csv\"\n",
    "data.to_csv(data_with_values_csv_file_path, index=False)\n",
    "\n",
    "print(\"Individualizing and moral_present values added to the data. Data saved to\", data_with_values_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment values added to the data. Data saved to data_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "########### Sentiment analysis - VADER ######\n",
    "\n",
    "# compound score is computed by summing the valence scores of each word \n",
    "#in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). \n",
    "#This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence.\n",
    "# Calling it a 'normalized, weighted composite score' is accurate.\n",
    "#It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either \n",
    "#positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "# positive sentiment: compound score >= 0.05\n",
    "# neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "# negative sentiment: compound score <= -0.05\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load your cleaned CSV file into a DataFrame\n",
    "cleaned_csv_file_path = \"data_with_moral_values.csv\"\n",
    "data = pd.read_csv(cleaned_csv_file_path)\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to classify sentiment based on compound score\n",
    "def classify_sentiment(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 2  # Positive sentiment\n",
    "    elif -0.05 < compound_score < 0.05:\n",
    "        return 1  # Neutral sentiment\n",
    "    else:\n",
    "        return 0  # Negative sentiment\n",
    "\n",
    "# Filter out NaN values from the 'text' column\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "# Calculate sentiment for each text and create a new column\n",
    "data['VADER_cont'] = data['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "data['VADER_cat'] = data['VADER_cont'].apply(classify_sentiment)\n",
    "\n",
    "# Save the updated data with sentiment values to a new CSV file\n",
    "data_with_sentiment_csv_file_path = \"data_with_sentiment.csv\"\n",
    "data.to_csv(data_with_sentiment_csv_file_path, index=False)\n",
    "\n",
    "print(\"Sentiment values added to the data. Data saved to\", data_with_sentiment_csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Experimenting with word clouds####\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# Filter data based on sentiment (replace 'positive' and 'negative' with your actual column names)\n",
    "positive_text = \" \".join(data[data['VADER_cat'] == 2]['text'])\n",
    "negative_text = \" \".join(data[data['VADER_cat'] == 0]['text'])\n",
    "\n",
    "# Initialize WordCloud object with custom stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update(['br', 'film', 'movie'])  # Update stopwords\n",
    "wc = WordCloud(background_color='white', max_words=50, stopwords=stopwords)\n",
    "\n",
    "# Generate and plot word cloud for positive sentiment\n",
    "positive_wordcloud = wc.generate(positive_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.title('Positive Sentiment')\n",
    "plt.axis('off')\n",
    "\n",
    "# Generate and plot word cloud for negative sentiment\n",
    "negative_wordcloud = wc.generate(negative_text)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Negative Sentiment')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Experimenting with word clouds####\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# Filter data based on sentiment (replace 'positive' and 'negative' with your actual column names)\n",
    "liberal_text = \" \".join(data[data['binary_indiv'] == 2]['text'])\n",
    "conservative_text = \" \".join(data[data['binary_indiv'] == 1]['text'])\n",
    "\n",
    "# Initialize WordCloud object with custom stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update(['br', 'film', 'movie'])  # Update stopwords\n",
    "wc = WordCloud(background_color='white', max_words=50, stopwords=stopwords)\n",
    "\n",
    "# Generate and plot word cloud for positive sentiment\n",
    "Liberal_wordcloud = wc.generate(liberal_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(Liberal_wordcloud, interpolation='bilinear')\n",
    "plt.title('Liberal')\n",
    "plt.axis('off')\n",
    "\n",
    "# Generate and plot word cloud for negative sentiment\n",
    "Conservative_wordcloud = wc.generate(conservative_text)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(Conservative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Conservative')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
